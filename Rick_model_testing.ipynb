{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple Sentiment on Twitter: 2013 vs 2023\n",
    "By Sarah Prusaitis, Rick Lataille, and Allison Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "- 2013 SXSW festival was a big success, very positive responses to new products\n",
    "- What was the nature of that positive response, what did people like\n",
    "- 10 years later, what is the public's response to Apple's new products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "- Has Apple maintained the public's support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Limitations\n",
    "- language shifts over time\n",
    "- dataset only includes pos/neg, no neutral examples\n",
    "- working with limited vocabulary, more robust approach would train on larger dataset\n",
    "- pos/neg sentiment may reflect not just apple, but tech in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "- Using 2013 dataset, tagged positive or negative by human raters\n",
    "    - Also including 10% of more recent tweets, labeled with VADER, to broaden vocabulary\n",
    "- Applying fitted models to new Vision Pro datasets to determine sentiment balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I ADDED ITERTOOLS IN THE NEXT CELL<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "import re\n",
    "import string\n",
    "import langid\n",
    "import itertools\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, regexp_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original data\n",
    "# df = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read synthetic data\n",
    "# df2 = pd.read_csv('data/Apple_Product_Negative_ Tweets_Sheet1.csv', encoding = 'ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I ADDED THE NEXT SEVERAL CELLS<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Vision Pro data\n",
    "df_vp = pd.read_csv('data/vision_pro_sentiment.csv',\n",
    "                    encoding='ISO-8859-1',\n",
    "                    usecols=['tweetText', 'mark']\n",
    "                   ).rename(columns={'tweetText': 'tweet', 'mark':'sentiment'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "- Converting emoticons to unique strings\n",
    "- Removing semantically meaningless patterns (mentions, links, etc)\n",
    "- Adding limited additional stopwords that likely have no semantic meaning\n",
    "- Tokenization, POS-tagging and Lemmatization\n",
    "- TF-IDF Vectorization and Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only English-language tweets, then drop language column\n",
    "df_vp['language'] = df_vp['tweet'].apply(lambda x: langid.classify(x)[0])\n",
    "df_english = df_vp.loc[df_vp['language']=='en'].drop('language',axis=1).copy()\n",
    "df_english.to_csv('Data/english_vp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take 10% of Vision Pro tweets for training purposes\n",
    "# df_english_train = df_english.sample(n=round(len(df_vp)*.1))\n",
    "# df_english.drop(df_english_train.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Vision Pro labels to 1, 0\n",
    "convert = {'Positive emotion':1, 'Negative emotion':0}\n",
    "df_english['sentiment'] = df_english['sentiment'].map(convert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename columns for simplicity\n",
    "# df = df.rename(columns = {'tweet_text': 'tweet', \n",
    "#                          'emotion_in_tweet_is_directed_at': 'product', \n",
    "#                          'is_there_an_emotion_directed_at_a_brand_or_product': 'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine rows into a single DataFrame\n",
    "# df = pd.concat([df1, df2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combined and renamed Apple products and non Apple products \n",
    "\n",
    "# df['product'] = df['product'].replace({\n",
    "#     'iPad': 'Apple',\n",
    "#     'Apple': 'Apple',\n",
    "#     'iPad or iPhone App': 'Apple',\n",
    "#     'iPhone': 'Apple',\n",
    "#     'Other Apple product or service': 'Apple',\n",
    "#     'Google': 'Other',\n",
    "#     'Other Google product or service': 'Other',\n",
    "#     'Android App': 'Other',\n",
    "#     'Android': 'Other'\n",
    "# })\n",
    "# #there are 5802 rows that are null - what should we do with those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I THINK I MOVED THIS UP IN ORDER TO DROP AS MANY TWEETS AS EARLY AS POSSIBLE (MAKES SUBSEQUENT CODE RUN FASTER)<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter DataFrame for only Apple tweets, and drop 'product column'\n",
    "# df_apple = df[df['product']=='Apple'].drop('product',axis=1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bring in Vision Pro tweets\n",
    "# df_apple = pd.concat([df_apple, df_english_train], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate no emotion entries, and drop\n",
    "# df_apple['sentiment'] = df_apple['sentiment'].replace(\"I can't tell\", \"No emotion toward brand or product\")\n",
    "# df_apple = df_apple.drop(df_apple[df_apple['sentiment'] == 'No emotion toward brand or product'].index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_apple['tweet'] = df_apple['tweet'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_emoticons(text):\n",
    "    # Define a dictionary mapping emoticons to their corresponding meanings\n",
    "    emoticon_mapping = {\n",
    "        ':D': 'emojismile',\n",
    "        ':)': 'emojismile',\n",
    "        ':-D': 'emojismile',\n",
    "        ':\\'': 'emojiunsure',\n",
    "        ':p': 'emojitongue',\n",
    "        ':P': 'emojitongue',\n",
    "        ':(': 'emojisad'\n",
    "        # Add more emoticons and their meanings as needed\n",
    "    }\n",
    "    pattern = re.compile('|'.join(re.escape(emoticon) for emoticon in emoticon_mapping.keys()))\n",
    "    \n",
    "    def replace(match):\n",
    "        return emoticon_mapping[match.group(0)]\n",
    "\n",
    "    return pattern.sub(replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I ADDED A LINE BELOW<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace emoticons with mapped strings\n",
    "# df_apple['tweet'] = df_apple['tweet'].apply(replace_emoticons)\n",
    "df_english['tweet'] = df_english['tweet'].apply(replace_emoticons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  NEW\n",
    "\n",
    "# Define stopwords\n",
    "additional_stopwords = {'w', 'u', 'amp', 'sxsw', 'rt', 'apple', 'sxswi', 'ipad', 'iphone', 'store'}  # amp = & \n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(additional_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    # Remove links and mentions\n",
    "    tweet = re.sub(r'http\\S+|@\\S+', '', tweet)\n",
    "    \n",
    "    # Remove {link}\n",
    "    tweet = re.sub(r'\\{link\\}', '', tweet)\n",
    "    \n",
    "    # Replace &quot; with \"\n",
    "    tweet = tweet.replace('&quot;', '\"')\n",
    "    \n",
    "    # Remove extra space between quotation mark and words\n",
    "    tweet = re.sub(r'\\s+\"', '\"', tweet)\n",
    "    tweet = re.sub(r'\"\\s+', '\"', tweet)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove numbers\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(r'([^\\w\\s]|_)+', ' ', tweet)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    \n",
    "    # Part-of-speech tagging\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, pos in tagged_tokens:\n",
    "        if pos.startswith('J'):\n",
    "            pos = 'a'  # Adjective\n",
    "        elif pos.startswith('V'):\n",
    "            pos = 'v'  # Verb\n",
    "        elif pos.startswith('N'):\n",
    "            pos = 'n'  # Noun\n",
    "        elif pos.startswith('R'):\n",
    "            pos = 'r'  # Adverb\n",
    "        else:\n",
    "            pos = 'n'  # Default to noun\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tweet = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I ADDED A LINE BELOW<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all tweets\n",
    "# df_apple['tweet'] = df_apple['tweet'].astype(str).apply(preprocess_tweet)\n",
    "df_english['tweet'] = df_english['tweet'].astype(str).apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label target with 1's and 0's\n",
    "# df_apple['target'] = df_apple['sentiment'].map({'Positive emotion': 1, 'Negative emotion': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW\n",
    "df_english = df_english.loc[df_english['sentiment']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train test split\n",
    "# X = df_apple['tweet']\n",
    "# y = df_apple['target'] # Target\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized data for Doc2Vec vectorizer\n",
    "# tokenized_train_data = X_train\n",
    "# tokenized_test_data = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized tweets back into strings for TfidfVectorizer\n",
    "# X_train_str = X_train.apply(lambda x: ' '.join(x))\n",
    "# X_test_str = X_test.apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####  NEW\n",
    "# tweets = pd.concat([X_train_str, X_test_str])\n",
    "tweets = df_english['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  NEW\n",
    "token_list = []\n",
    "for tweet in tweets:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    token_list.extend(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NEW\n",
    "\n",
    "fdist_train = FreqDist(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of tokens: 122591\n",
      "Total Unique tokens: 10855\n",
      "Frequency of 'pop': 7\n",
      "Most Common Words: [('applevisionpro', 6258), ('vision', 3854), ('ð', 3848), ('pro', 2945), ('â', 1090), ('like', 899), ('vr', 868), ('visionpro', 719), ('new', 663), ('ar', 572), ('see', 553), ('future', 520), ('tech', 512), ('love', 485), ('app', 472), ('get', 467), ('experience', 455), ('quest', 451), ('ï', 450), ('spatial', 448), ('would', 440), ('one', 381), ('headset', 378), ('spatialcomputing', 371), ('zombies', 371), ('win', 365), ('technology', 361), ('jup', 354), ('gme', 351), ('first', 337), ('exciting', 334), ('amazing', 331), ('visionos', 319), ('solana', 318), ('time', 312), ('video', 307), ('meta', 305), ('apps', 302), ('ai', 300), ('innovation', 300), ('best', 298), ('world', 298), ('reality', 297), ('also', 290), ('free', 281), ('share', 274), ('immersive', 271), ('great', 271), ('better', 270), ('use', 265), ('digital', 260), ('virtualreality', 258), ('ready', 258), ('way', 255), ('wait', 251), ('people', 246), ('itâ', 243), ('create', 241), ('iâ', 237), ('us', 237), ('xr', 235), ('good', 235), ('metaquest', 233), ('computing', 225), ('device', 225), ('via', 222), ('think', 218), ('using', 218), ('day', 207), ('game', 205), ('make', 201), ('metaverse', 201), ('tag', 195), ('news', 192), ('look', 191), ('help', 190), ('opportunity', 190), ('launch', 188), ('virtual', 181), ('today', 179), ('experiences', 177), ('want', 177), ('check', 176), ('next', 174), ('days', 172), ('fun', 171), ('friend', 171), ('watch', 169), ('could', 164), ('available', 160), ('real', 159), ('really', 158), ('spread', 158), ('x', 156), ('cool', 155), ('latest', 154), ('work', 154), ('even', 154), ('know', 152), ('robloxdevs', 152), ('content', 151), ('fursuitfriday', 151), ('prizeâ', 150), ('palworldxboxsweepstakes', 150), ('avp', 148), ('hand', 148), ('technews', 147), ('got', 146), ('excited', 145), ('man', 142), ('live', 141), ('features', 140), ('review', 140), ('buy', 139), ('read', 138), ('need', 138), ('take', 137), ('well', 137), ('design', 137), ('channel', 136), ('let', 136), ('price', 134), ('soon', 134), ('much', 133), ('subscribe', 133), ('eye', 133), ('gaming', 131), ('ever', 131), ('join', 131), ('space', 131), ('going', 131), ('yet', 130), ('augmentedreality', 125), ('product', 125), ('youtube', 124), ('still', 123), ('coming', 123), ('try', 121), ('around', 121), ('dive', 119), ('making', 119), ('already', 118), ('support', 118), ('demo', 117), ('automated', 117), ('lobotomies', 115), ('rage', 115), ('talented', 115), ('discovered', 115), ('feel', 115), ('dissolve', 114), ('cage', 114), ('tracking', 114), ('super', 114), ('entertainment', 113), ('looks', 112), ('never', 111), ('mixed', 111), ('thanks', 110), ('right', 110), ('soraai', 109), ('life', 109), ('things', 109), ('users', 109), ('k', 108), ('thoughts', 108), ('yes', 107), ('donâ', 107), ('say', 106), ('go', 105), ('release', 105), ('come', 104), ('many', 104), ('web', 103), ('ios', 102), ('groundbreaking', 102), ('mixedreality', 101), ('years', 101), ('made', 100), ('screen', 100), ('full', 100), ('headsets', 100), ('wow', 99), ('discover', 99), ('comedy', 98), ('automation', 98), ('zuckerberg', 98), ('pre', 98), ('awesome', 97), ('set', 97), ('looking', 96), ('week', 96), ('applevision', 95), ('part', 94), ('big', 94), ('market', 94), ('huge', 93), ('thing', 93), ('february', 93), ('tv', 92), ('tried', 92), ('something', 91), ('used', 91), ('find', 91), ('appleâ', 91), ('sure', 90), ('art', 89), ('actually', 89), ('pretty', 89), ('always', 88), ('movies', 88), ('high', 87), ('back', 87), ('videos', 87), ('team', 86), ('stay', 86), ('user', 85), ('impressive', 85), ('play', 84), ('mark', 84), ('incredible', 83), ('explore', 83), ('crypto', 83), ('½', 81), ('potential', 81), ('computer', 81), ('lot', 81), ('watching', 81), ('business', 80), ('since', 79), ('far', 79), ('games', 78), ('tiktok', 78), ('thank', 78), ('may', 78), ('working', 77), ('feature', 77), ('top', 76), ('mr', 76), ('hands', 75), ('long', 75), ('seen', 74), ('getting', 73), ('year', 73), ('music', 73), ('learn', 73), ('easy', 73), ('ovr', 73), ('project', 73), ('perfect', 73)]\n"
     ]
    }
   ],
   "source": [
    "#### NEW\n",
    "\n",
    "print(f'Total Number of tokens: {fdist_train.N()}')\n",
    "print(f'Total Unique tokens: {fdist_train.B()}')\n",
    "print(f\"Frequency of 'pop': {fdist_train['pop']}\")\n",
    "most_common_words = fdist_train.most_common(250)\n",
    "print(f\"Most Common Words: {most_common_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['applevisionpro',\n",
       " 'vision',\n",
       " 'ð',\n",
       " 'pro',\n",
       " 'â',\n",
       " 'like',\n",
       " 'vr',\n",
       " 'visionpro',\n",
       " 'new',\n",
       " 'ar',\n",
       " 'see',\n",
       " 'future',\n",
       " 'tech',\n",
       " 'love',\n",
       " 'app',\n",
       " 'get',\n",
       " 'experience',\n",
       " 'quest',\n",
       " 'ï',\n",
       " 'spatial',\n",
       " 'would',\n",
       " 'one',\n",
       " 'headset',\n",
       " 'spatialcomputing',\n",
       " 'zombies',\n",
       " 'win',\n",
       " 'technology',\n",
       " 'jup',\n",
       " 'gme',\n",
       " 'first',\n",
       " 'exciting',\n",
       " 'amazing',\n",
       " 'visionos',\n",
       " 'solana',\n",
       " 'time',\n",
       " 'video',\n",
       " 'meta',\n",
       " 'apps',\n",
       " 'ai',\n",
       " 'innovation',\n",
       " 'best',\n",
       " 'world',\n",
       " 'reality',\n",
       " 'also',\n",
       " 'free',\n",
       " 'share',\n",
       " 'immersive',\n",
       " 'great',\n",
       " 'better',\n",
       " 'use',\n",
       " 'digital',\n",
       " 'virtualreality',\n",
       " 'ready',\n",
       " 'way',\n",
       " 'wait',\n",
       " 'people',\n",
       " 'itâ',\n",
       " 'create',\n",
       " 'iâ',\n",
       " 'us',\n",
       " 'xr',\n",
       " 'good',\n",
       " 'metaquest',\n",
       " 'computing',\n",
       " 'device',\n",
       " 'via',\n",
       " 'think',\n",
       " 'using',\n",
       " 'day',\n",
       " 'game',\n",
       " 'make',\n",
       " 'metaverse',\n",
       " 'tag',\n",
       " 'news',\n",
       " 'look',\n",
       " 'help',\n",
       " 'opportunity',\n",
       " 'launch',\n",
       " 'virtual',\n",
       " 'today',\n",
       " 'experiences',\n",
       " 'want',\n",
       " 'check',\n",
       " 'next',\n",
       " 'days',\n",
       " 'fun',\n",
       " 'friend',\n",
       " 'watch',\n",
       " 'could',\n",
       " 'available',\n",
       " 'real',\n",
       " 'really',\n",
       " 'spread',\n",
       " 'x',\n",
       " 'cool',\n",
       " 'latest',\n",
       " 'work',\n",
       " 'even',\n",
       " 'know',\n",
       " 'robloxdevs',\n",
       " 'content',\n",
       " 'fursuitfriday',\n",
       " 'prizeâ',\n",
       " 'palworldxboxsweepstakes',\n",
       " 'avp',\n",
       " 'hand',\n",
       " 'technews',\n",
       " 'got',\n",
       " 'excited',\n",
       " 'man',\n",
       " 'live',\n",
       " 'features',\n",
       " 'review',\n",
       " 'buy',\n",
       " 'read',\n",
       " 'need',\n",
       " 'take',\n",
       " 'well',\n",
       " 'design',\n",
       " 'channel',\n",
       " 'let',\n",
       " 'price',\n",
       " 'soon',\n",
       " 'much',\n",
       " 'subscribe',\n",
       " 'eye',\n",
       " 'gaming',\n",
       " 'ever',\n",
       " 'join',\n",
       " 'space',\n",
       " 'going',\n",
       " 'yet',\n",
       " 'augmentedreality',\n",
       " 'product',\n",
       " 'youtube',\n",
       " 'still',\n",
       " 'coming',\n",
       " 'try',\n",
       " 'around',\n",
       " 'dive',\n",
       " 'making',\n",
       " 'already',\n",
       " 'support',\n",
       " 'demo',\n",
       " 'automated',\n",
       " 'lobotomies',\n",
       " 'rage',\n",
       " 'talented',\n",
       " 'discovered',\n",
       " 'feel',\n",
       " 'dissolve',\n",
       " 'cage',\n",
       " 'tracking',\n",
       " 'super',\n",
       " 'entertainment',\n",
       " 'looks',\n",
       " 'never',\n",
       " 'mixed',\n",
       " 'thanks',\n",
       " 'right',\n",
       " 'soraai',\n",
       " 'life',\n",
       " 'things',\n",
       " 'users',\n",
       " 'k',\n",
       " 'thoughts',\n",
       " 'yes',\n",
       " 'donâ',\n",
       " 'say',\n",
       " 'go',\n",
       " 'release',\n",
       " 'come',\n",
       " 'many',\n",
       " 'web',\n",
       " 'ios',\n",
       " 'groundbreaking',\n",
       " 'mixedreality',\n",
       " 'years',\n",
       " 'made',\n",
       " 'screen',\n",
       " 'full',\n",
       " 'headsets',\n",
       " 'wow',\n",
       " 'discover',\n",
       " 'comedy',\n",
       " 'automation',\n",
       " 'zuckerberg',\n",
       " 'pre',\n",
       " 'awesome',\n",
       " 'set',\n",
       " 'looking',\n",
       " 'week',\n",
       " 'applevision',\n",
       " 'part',\n",
       " 'big',\n",
       " 'market',\n",
       " 'huge',\n",
       " 'thing',\n",
       " 'february',\n",
       " 'tv',\n",
       " 'tried',\n",
       " 'something',\n",
       " 'used',\n",
       " 'find',\n",
       " 'appleâ',\n",
       " 'sure',\n",
       " 'art',\n",
       " 'actually',\n",
       " 'pretty',\n",
       " 'always',\n",
       " 'movies',\n",
       " 'high',\n",
       " 'back',\n",
       " 'videos',\n",
       " 'team',\n",
       " 'stay',\n",
       " 'user',\n",
       " 'impressive',\n",
       " 'play',\n",
       " 'mark',\n",
       " 'incredible',\n",
       " 'explore',\n",
       " 'crypto',\n",
       " '½',\n",
       " 'potential',\n",
       " 'computer',\n",
       " 'lot',\n",
       " 'watching',\n",
       " 'business',\n",
       " 'since',\n",
       " 'far',\n",
       " 'games',\n",
       " 'tiktok',\n",
       " 'thank',\n",
       " 'may',\n",
       " 'working',\n",
       " 'feature',\n",
       " 'top',\n",
       " 'mr',\n",
       " 'hands',\n",
       " 'long',\n",
       " 'seen',\n",
       " 'getting',\n",
       " 'year',\n",
       " 'music',\n",
       " 'learn',\n",
       " 'easy',\n",
       " 'ovr',\n",
       " 'project',\n",
       " 'perfect']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####  NEW\n",
    "\n",
    "[item[0] for item in most_common_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorization\n",
    "- Most common approach, more meaning than simple bag-of-words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I CHANGED SOME OF THE INSTANTIATION ARGUMENTS<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_str' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-e0124aefefef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Fit and transform the vectorizer on the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX_train_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mX_test_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_str' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform the vectorizer on the training data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_str)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: THIS CELL IS NEW, IT'S NEEDED IN ORDER TO RESTRICT VOCAB ON VISION PRO DATA<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vocab for vision pro analysis and retrain\n",
    "vocab = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1,2), vocabulary=vocab)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_str)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec Vectorization\n",
    "- This is a more sophisticated approach, captures additional meaning from document and word context\n",
    "- GBM models can make use of this approach\n",
    "- May not be better for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag tokenized_tweets with an index for identification\n",
    "tagged_train_data = [TaggedDocument(doc, [i]) for i, doc in enumerate(tokenized_train_data)]\n",
    "\n",
    "# Initialize and train a Doc2Vec vectorizer\n",
    "vectorizer = Doc2Vec(tagged_train_data, vector_size=50, window=2, min_count=1, workers=4, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer vectors for testing set\n",
    "test_vectors = np.array([vectorizer.infer_vector(doc_tokens) for doc_tokens in tokenized_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.array([vectorizer.dv[i] for i in range(len(tagged_train_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "- Many potentially good models, not clear which one will work best\n",
    "- Trying 5\n",
    "    - Logistic Regression\n",
    "    - Multinomial Naive Bayes\n",
    "    - Support Vector Machines\n",
    "    - Random Forest Classifier\n",
    "    - LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- Performs well with binomial classification tasks\n",
    "- Interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_lr = logreg.predict(X_train_tfidf)\n",
    "y_test_preds_lr = logreg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LogReg Train Accuracy: {accuracy_score(y_train, y_preds_lr):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_test_preds_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LogReg Test Accuracy: {accuracy_score(y_test,y_test_preds_lr):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes\n",
    "- Good for multinomial classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I THINK THERE WERE SOME TYPOS IN THE MULTI NB SECTION, I FIXED<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_mnb = mnb.predict(X_train_tfidf)\n",
    "y_preds_test_mnb = mnb.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_preds_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MultinomialNB Train Accuracy: {accuracy_score(y_train, y_preds_mnb):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_preds_test_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MultinomialNB Test Accuracy: {accuracy_score(y_test, y_preds_test_mnb):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "- Typically performs better in image or text classification task\n",
    "- Not interpretable\n",
    "- May not work as well on data with new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: I COMMENTED OUT INITIAL AND GRIDSEARCH CODE TO SPEED UP THE MODEL, WE CAN UNCOMMENT FOR THE FINAL AND LET IT RUN<span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(random_state=42)\n",
    "# svc.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds_svc = svc.predict(X_train_tfidf)\n",
    "# y_preds_test_svc = svc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_train, y_preds_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_preds_test_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#           'degree':[2,3,4],\n",
    "#           'shrinking':[True,False],\n",
    "#          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc_grid = GridSearchCV(svc, param_grid=params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# svc_grid.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(svc_grid.best_estimator_)\n",
    "# print(svc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tuned = SVC(degree=2, kernel='poly', shrinking=True, random_state=42)\n",
    "svc_tuned.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_svc = svc_tuned.predict(X_train_tfidf)\n",
    "y_preds_test_svc = svc_tuned.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_preds_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Train Accuracy: {accuracy_score(y_train, y_preds_svc):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_preds_test_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Test Accuracy: {accuracy_score(y_test, y_preds_test_svc):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "- Might be better for non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds_rf = rf.predict(X_train_tfidf)\n",
    "# y_preds_test_rf = rf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_train, y_preds_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_test, y_preds_test_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_params = {'n_estimators':[10, 50, 100],\n",
    "#              'criterion':['gini','entropy','log_loss'],\n",
    "#              'max_depth':[5,10,20]\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_grid = GridSearchCV(rf, param_grid=rf_params, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf_grid.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rf_grid.best_estimator_)\n",
    "# print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier(criterion='log_loss', max_depth=20, n_estimators=100, random_state=42)\n",
    "rf2.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds_rf2 = rf2.predict(X_train_tfidf)\n",
    "y_preds_test_rf2 = rf2.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train, y_preds_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Train Accuracy: {accuracy_score(y_train, y_preds_rf2):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_preds_test_rf2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random Forest Test Accuracy: {accuracy_score(y_test, y_preds_test_rf2):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LightGBM\n",
    "- Performs very well on very large datasets\n",
    "- Excels at detecting complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = lgb.Dataset(vectors, label=y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'boosting_type': 'gbdt',  # Traditional Gradient Boosting Decision Tree\n",
    "          'objective': 'binary',    # Binary classification\n",
    "          'metric': ['binary_error'],  # Evaluation metrics\n",
    "          'lambda_l1': 0.5,\n",
    "          'lambda_l2': 0.5,\n",
    "          'max_bin': 100,\n",
    "          'num_leaves': 20,         # Number of leaves in full trees\n",
    "          'learning_rate': 0.05,    # Learning rate\n",
    "          'feature_fraction': 0.9,  # Fraction of features to be used at each iteration\n",
    "          'bagging_fraction': 0.8,  # Fraction of data to be used for each iteration\n",
    "          'bagging_freq': 5,        # Frequency for bagging\n",
    "          'verbose': 1              # Verbose output in the terminal\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_round = 100  # Number of boosting rounds\n",
    "lgb_model = lgb.train(params, train_data, num_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and convert to binary\n",
    "y_preds_lgb = lgb_model.predict(vectors, num_iteration=lgb_model.best_iteration)\n",
    "y_preds_binary = [1 if prob > 0.5 else 0 for prob in y_preds_lgb]\n",
    "\n",
    "print(classification_report(y_train, y_preds_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LightGBM Train Accuracy: {accuracy_score(y_train, y_preds_binary):.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_preds_test_lgb = lgb_model.predict(test_vectors, num_iteration=lgb_model.best_iteration)\n",
    "y_preds_test_binary = [1 if prob > 0.5 else 0 for prob in y_preds_test_lgb]\n",
    "\n",
    "print(classification_report(y_test, y_preds_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LightGBM Test Accuracy: {accuracy_score(y_test, y_preds_test_binary):.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Conclusion\n",
    "- LightGBM is bad\n",
    "- SVM seems badly overfit\n",
    "- Multinomial NB and Random Forest are okay\n",
    "- Logistic Regression is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: red;\">NOTE: EVERYTHING BELOW IS NEW<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize vision pro tweets\n",
    "- Use the *fitted* vectorizer to vectorize the Vision Pro tweets\n",
    "- The vectorizer must be limited to only the vocabulary seen in the initial dataset\n",
    "- Words not in the initial dataset will be dropped, which will limit the model's accuracy on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized tweets back into strings for TfidfVectorizer\n",
    "df_english['tweet_join'] = df_english['tweet'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the new tweets with the fitted vectorizer\n",
    "vp_vectored = tfidf_vectorizer.fit_transform(df_english['tweet_join'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "vp_preds = logreg.predict(vp_vectored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(df_english['sentiment'], vp_preds)\n",
    "ConfusionMatrixDisplay(cm).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_english['sentiment'], vp_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'LogReg Accuracy vs VADER: {accuracy_score(df_english[\"sentiment\"], vp_preds):.1%}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
